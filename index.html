<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="robots" content="noindex, nofollow, noarchive, nosnippet"> <title> Junzhi (Raymond) Ning </title> <meta name="author" content="Junzhi (Raymond) Ning"> <meta name="description" content="Portrait of a Man in Quest of the unknown, yet satisfied. MRes in Machine Learning at Imperial College London. "> <meta name="keywords" content="machine-learning, medical-imaging, artificial-intelligence, computer-vision, deep-learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/optimized_solution.jpg?ca2ddf899b29608d36c5260cf0b4a94a"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://junzhin.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Junzhi</span> (Raymond) Ning </h1> <p class="desc">Machine Learning Researcher at Shanghai AI Lab</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimized_solution-480.webp 480w,/assets/img/optimized_solution-800.webp 800w,/assets/img/optimized_solution-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/optimized_solution.jpg?ca2ddf899b29608d36c5260cf0b4a94a" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="optimized_solution.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>Machine Learning Researcher</p> <p><a href="https://www.shlab.org.cn/" rel="external nofollow noopener" target="_blank">Shanghai AI Lab</a>, Shanghai, China</p> <p>Supervised by <a href="https://scholar.google.com/citations?user=Z4LgebkAAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank">Dr. Junjun He</a></p> <p>MRes Graduate from Imperial College London</p> </div> </div> <div class="clearfix"> <p>I am a Machine Learning Researcher in the <strong>General Medical AI (GMAI)</strong> group at <a href="https://www.shlab.org.cn/" rel="external nofollow noopener" target="_blank"><strong>Shanghai AI Lab</strong></a>, supervised by <a href="https://scholar.google.com/citations?user=Z4LgebkAAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank"><strong>Dr. Junjun He</strong></a>. My research focuses on <strong>generative AI</strong> and <strong>multimodal learning</strong> for medical applications, specializing in large-scale synthetic data generation and deep generative models. I work on developing scalable workflows to create millions of high-quality medical training samples, addressing critical challenges in data scarcity and domain adaptation for healthcare AI.</p> <p>I completed my MRes with <strong>Distinction</strong> at Imperial College London (Oct 2023 - Oct 2024), Supervised by <a href="https://scholar.google.com/citations?user=xpAYtroAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><strong>Dr. Matthieu Komorowski</strong></a> and <a href="https://scholar.google.com/citations?user=ZfzEFpsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><strong>Dr. Guang Yang</strong></a> jointly, I developed deep generative models for chest X-ray image translation to improve diagnostic accuracy. During this period, I collaborated with ICU clinicians and contributed to research proposals for industrial funding.</p> <p>My educational background includes a Bachelor of Science (Honours) in Data Science with <strong>University Medal</strong> from The University of Sydney, a concurrent Diploma in Computing, and a Bachelor of Science in Mathematics and Statistics from The University of Melbourne (First-Class Honours).</p> <hr> <div class="education-affiliations" style="clear: both; margin: 40px 0; padding: 20px 0; border-top: 1px solid #e0e0e0; border-bottom: 1px solid #e0e0e0;"> <h3 style="font-size: 1.5rem; font-weight: 600; margin-bottom: 25px; color: #555; text-align: center;">Academic Affiliations</h3> <div style="display: flex; justify-content: space-between; align-items: center; width: 100%;"> <a href="https://www.sydney.edu.au/" target="_blank" title="University of Sydney" style="flex: 1; text-align: center; transition: transform 0.2s;" rel="external nofollow noopener"> <img src="/assets/img/usyd_logo.svg" alt="University of Sydney" style="max-height: 80px; max-width: 90%; object-fit: contain;"> </a> <a href="https://www.unimelb.edu.au/" target="_blank" title="University of Melbourne" style="flex: 1; text-align: center; transition: transform 0.2s;" rel="external nofollow noopener"> <img src="/assets/img/unimelb_logo.svg" alt="University of Melbourne" style="max-height: 80px; max-width: 90%; object-fit: contain;"> </a> <a href="https://www.imperial.ac.uk/" target="_blank" title="Imperial College London" style="flex: 1; text-align: center; transition: transform 0.2s;" rel="external nofollow noopener"> <img src="/assets/img/imperial_logo.png" alt="Imperial College London" style="max-height: 80px; max-width: 90%; object-fit: contain;"> </a> <a href="https://www.shlab.org.cn/" target="_blank" title="Shanghai AI Lab" style="flex: 1; text-align: center; transition: transform 0.2s;" rel="external nofollow noopener"> <img src="/assets/img/shanghai_ailab_logo.png" alt="Shanghai AI Lab" style="max-height: 80px; max-width: 90%; object-fit: contain;"> </a> </div> </div> </div> <div class="publication-summary" style="margin: 35px -20px 30px -20px; padding: 25px 40px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); box-shadow: 0 4px 12px rgba(0,0,0,0.15);"> <h3 style="color: white; font-size: 1.4rem; font-weight: 700; margin-bottom: 18px; text-align: center; text-shadow: 2px 2px 4px rgba(0,0,0,0.2); letter-spacing: 0.5px;">📚 Publications Summary</h3> <p style="color: white; font-size: 1rem; font-weight: 500; margin: 0; line-height: 2; text-align: center;"> 4× MICCAI 2025 <span style="opacity: 0.9;">(1 oral, 1 spotlight)</span> • 1× WACV 2025 • 1× IJCAI 2024 • 1× ISBI 2025 • 1× PRL • 1× NeurIPS Workshop <span style="opacity: 0.9;">(oral)</span> • 3× Under Review <span style="opacity: 0.9;">(arXiv/Tech Report)</span> </p> </div> <h2> <a style="color: inherit">🔬 Research Focus</a> </h2> <div class="research-focus"> <p>My current research focuses on advancing <strong>Generative AI</strong> for healthcare applications, with specific achievements in:</p> <ul> <li> <strong>Multimodal Medical AI</strong>: Contributing to GMAI-VL-R1 (RL-enhanced medical reasoning) and vision-language models for text-guided medical image generation and analysis</li> <li> <strong>Large-scale Synthetic Data Generation</strong>: Contributing to RetinaLogos-1400k dataset (1.4M synthetic retinal images) and scalable workflows for generating millions of high-quality medical training samples</li> <li> <strong>Deep Generative Models</strong>: Contributing to medical image translation, including chest X-ray opacity removal and anatomical structure enhancement</li> </ul> <p>I have contributed to publications at top-tier venues including <strong>MICCAI 2025</strong> (4 papers: 1 oral, 1 spotlight), <strong>ISBI 2025</strong>, <strong>WACV 2025</strong>, <strong>IJCAI 2025</strong>, <strong>Pattern Recognition Letters</strong>, and <strong>NeurIPS Workshop</strong>.</p> </div> <h2> <a style="color: inherit">🏆 Awards &amp; Recognition</a> </h2> <div class="awards"> <ul> <li> <strong>University Medal</strong>, Bachelor of Science (Honours) in Data Science, University of Sydney (2023)</li> <li> <strong>Dean’s Honours List for Data Science</strong>, University of Sydney (2023)</li> <li> <strong>Melbourne International Undergraduate Scholarship</strong>, University of Melbourne (2022)</li> <li> <strong>Dean’s Honours List</strong>, University of Melbourne (2019)</li> </ul> </div> <h2> <a style="color: inherit">💼 Seeking Opportunities</a> </h2> <div class="seeking-opportunities"> <p><strong>I am actively seeking PhD positions for Fall 2025 and Spring 2026, as well as research internship opportunities.</strong></p> <p>Feel free to contact me for research opportunities and collaboration.</p> <hr> <p><em>“Positivity is the essence of progress. In every challenge, I see an opportunity for learning and growth.”</em></p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 17, 2025</th> <td> 🎉 <strong>First-author paper accepted at MICCAI 2025!</strong> RetinaLogos: A novel text-guided retinal image synthesis model with fine-grained anatomical control. 👁️ </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 17, 2025</th> <td> 🎉 <strong>4 papers accepted at MICCAI 2025!</strong> Including 1 first-author paper (RetinaLogos), 1 oral presentation (Ophora), and 1 spotlight (MedGround-R1). Proud to contribute to advancing medical AI research! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 28, 2025</th> <td> 📄 <strong>Cyclic Vision-Language Manipulator</strong> paper accepted at <strong>IJCAI 2025</strong>! Joint work on reliable image interpretation for automated report generation in medical imaging. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 15, 2025</th> <td> 📰 <strong>First-author paper accepted in Pattern Recognition Letters</strong>! Our work on unpaired chest X-ray translation for lung opacity diagnosis has been accepted. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 15, 2024</th> <td> 🎉 <strong>First-author paper accepted at ISBI 2025</strong>! “Unveiling the Capabilities of Latent Diffusion Models for Classification of Lung Diseases in Chest X-Rays” explores new frontiers in medical AI. </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 01, 2024</th> <td> 🚀 Started as Machine Learning Researcher at <strong>Shanghai AI Lab</strong>, focusing on multimodal medical AI models and large-scale synthetic dataset generation for healthcare applications. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 28, 2024</th> <td> 🏆 <strong>DMRN</strong> paper accepted at <strong>WACV 2025</strong>! A dynamical multi-order response network for robust lung airway segmentation in medical imaging. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 15, 2024</th> <td> 🎓 <strong>Graduated from Imperial College London</strong> with MRes in AI and Machine Learning (Distinction, First-Class Honours)! </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/retina_logos.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="retina_logos.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ning2025retinalogos" class="col-sm-8"> <div class="title">RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions</div> <div class="author"> <em>Junzhi Ning</em>, Cheng Tang, Kaijing Zhou, and <span class="more-authors" title="click to view 12 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '12 more authors' ? 'Diping Song, Lihao Liu, Ming Hu, Wei Li, Huihui Xu, Yanzhou Su, Tianbin Li, Jiyao Liu, Jin Ye, Sheng Zhang, Yuanfeng Ji, Junjun He' : '12 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">12 more authors</span> </div> <div class="periodical"> <em>MICCAI</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://anonymous.4open.science/r/Text-Driven-CFP-Generator" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The scarcity of high-quality, labelled retinal imaging data, which presents a significant challenge in the development of machine learning models for ophthalmology, hinders progress in the field. Existing methods for synthesising Colour Fundus Photographs (CFPs) largely rely on predefined disease labels, which restricts their ability to generate images that reflect fine-grained anatomical variations, subtle disease stages, and diverse pathological features beyond coarse class categories. To overcome these challenges, we first introduce an innovative pipeline that creates a large-scale, captioned retinal dataset comprising 1.4 million entries, called RetinaLogos-1400k. Specifically, RetinaLogos-1400k uses the visual language model(VLM) to describe retinal conditions and key structures, such as optic disc configuration, vascular distribution, nerve fibre layers, and pathological features. Building on this dataset, we employ a novel three-step training framework, RetinaLogos, which enables fine-grained semantic control over retinal images and accurately captures different stages of disease progression, subtle anatomical variations, and specific lesion types. Through extensive experiments, our method demonstrates superior performance across multiple datasets, with 62.07% of text-driven synthetic CFPs indistinguishable from real ones by ophthalmologists. Moreover, the synthetic data improves accuracy by 5%-10% in diabetic retinopathy grading and glaucoma detection. Codes are available at https://github.com/uni-medical/retina-text2cfp.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ning2025retinalogos</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{MICCAI}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ning, Junzhi and Tang, Cheng and Zhou, Kaijing and Song, Diping and Liu, Lihao and Hu, Ming and Li, Wei and Xu, Huihui and Su, Yanzhou and Li, Tianbin and Liu, Jiyao and Ye, Jin and Zhang, Sheng and Ji, Yuanfeng and He, Junjun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Medical Image Computing and Computer Assisted Intervention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ISBI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/latent_diffusion_cxr.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="latent_diffusion_cxr.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ning2025latent" class="col-sm-8"> <div class="title">Unveiling the Capabilities of Latent Diffusion Models for Classification of Lung Diseases in Chest X-Rays</div> <div class="author"> <em>Junzhi Ning</em>, Xiaodan Xing, Sheng Zhang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Xiao Ma, Guang Yang' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>ISBI</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2411.XXXXX" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Diffusion models have demonstrated remarkable ability in synthesizing chest X-ray (CXR) images, particularly by generating high-quality samples to address the scarcity and imbalance of annotated CXR datasets. While these models excel in generating realistic samples-suggesting that they contain rich discriminative information effectively harnessing this capability for disease classification and decomposition remains a challenge. This study investigates an approach that leverages latent conditional diffusion models, which are conditioned on corresponding radiology reports, for lung disease classification in CXRs. Specifically, we employ a pre-trained latent conditional diffusion model for CXRs to predict noise estimates for a noisy input lung CXR under various disease conditions. By comparing the noise estimation errors associated with different class prompts, we determine the most probable disease classification based on the minimal error. Through the experiments, we demonstrate that the CXR diffusion-based classifier not only achieves zero-shot classification performance comparable to existing models but also reveals lesion regions aligning with ground truth lesion areas in CXRs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ning2025latent</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ISBI}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unveiling the Capabilities of Latent Diffusion Models for Classification of Lung Diseases in Chest X-Rays}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ning, Junzhi and Xing, Xiaodan and Zhang, Sheng and Ma, Xiao and Yang, Guang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Symposium on Biomedical Imaging}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-5}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">PRL</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cxr_lung_opacity.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cxr_lung_opacity.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ning2025cxr" class="col-sm-8"> <div class="title">Unpaired translation of chest X-ray images for lung opacity diagnosis via adaptive activation masks and cross-domain alignment</div> <div class="author"> <em>Junzhi Ning</em>, Dominic C. Marshall, Yijian Gao, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Xiaodan Xing, Yang Nan, Yingying Fang, Sheng Zhang, Matthieu Komorowski, Guang Yang' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>Pattern Recognition Letters</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0167865525001254" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Chest X-ray radiographs (CXRs) play a pivotal role in diagnosing and monitoring cardiopulmonary diseases. However, lung opacities in CXRs frequently obscure anatomical structures, impeding clear identification of lung borders and complicating localisation of pathology. This challenge significantly hampers segmentation accuracy and precise lesion identification, crucial for diagnosis. To tackle these issues, our study proposes an unpaired CXR translation framework that converts CXRs with lung opacities into counterparts without lung opacities while preserving semantic features. Central to our approach is the use of adaptive activation masks to selectively modify opacity regions in lung CXRs. Cross-domain alignment ensures translated CXRs without opacity issues align with feature maps and prediction labels from a pre-trained CXR lesion classifier, facilitating the interpretability of the translation process. We validate our method using RSNA, MIMIC-CXR-JPG and JSRT datasets, demonstrating superior translation quality through lower Fréchet Inception Distance (FID) and Kernel Inception Distance (KID) scores compared to existing methods (FID: 67.18 vs. 210.4, KID: 0.01604 vs. 0.225). Evaluation on RSNA opacity, MIMIC acute respiratory distress syndrome (ARDS) patient CXRs and JSRT CXRs shows our method enhances segmentation accuracy of lung borders and improves lesion classification, further underscoring its potential in clinical settings (RSNA: mIoU: 76.58% vs. 62.58%, Sensitivity: 85.58% vs. 77.03%; MIMIC ARDS: mIoU: 86.20% vs. 72.07%, Sensitivity: 92.68% vs. 86.85%; JSRT: mIoU: 91.08% vs. 85.6%, Sensitivity: 97.62% vs. 95.04%). Our approach advances CXR imaging analysis, especially in investigating segmentation impacts through image translation techniques.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ning2025cxr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unpaired translation of chest X-ray images for lung opacity diagnosis via adaptive activation masks and cross-domain alignment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ning, Junzhi and Marshall, Dominic C. and Gao, Yijian and Xing, Xiaodan and Nan, Yang and Fang, Yingying and Zhang, Sheng and Komorowski, Matthieu and Yang, Guang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Pattern Recognition Letters}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{193}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{21-28}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Neurips Workshop</abbr> <figure> <picture> <img src="/assets/img/publication_preview/mask2ct.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mask2ct.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xing2024dgm" class="col-sm-8"> <div class="title">Deep Generative Models Unveil Patterns in Medical Images Through Vision-Language Conditioning</div> <div class="author"> Xiaodan Xing, <em>Junzhi Ning</em>, Yang Nan, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Guang Yang' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2410.13823" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/junzhin/DGM-VLC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Deep generative models have significantly advanced medical imaging analysis by enhancing dataset size and quality. Beyond mere data augmentation, our research in this paper highlights an additional, significant capacity of deep generative models: their ability to reveal and demonstrate patterns in medical images. We employ a generative structure with hybrid conditions, combining clinical data and segmentation masks to guide the image synthesis process. Furthermore, we innovatively transformed the tabular clinical data into textual descriptions. This approach simplifies the handling of missing values and also enables us to leverage large pre-trained vision-language models that investigate the relations between independent clinical entries and comprehend general terms, such as gender and smoking status. Our approach differs from and presents a more challenging task than traditional medical report-guided synthesis due to the less visual correlation of our clinical information with the images. To overcome this, we introduce a text-visual embedding mechanism that strengthens the conditions, ensuring the network effectively utilizes the provided information. Our pipeline is generalizable to both GAN-based and diffusion models. Experiments on chest CT, particularly focusing on the smoking status, demonstrated a consistent intensity shift in the lungs which is in agreement with clinical observations, indicating the effectiveness of our method in capturing and visualizing the impact of specific attributes on medical image patterns. Our methods offer a new avenue for the early detection and precise visualization of complex clinical conditions with deep generative models. All codes are this https URL. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xing2024dgm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Generative Models Unveil Patterns in Medical Images Through Vision-Language Conditioning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xing, Xiaodan and Ning, Junzhi and Nan, Yang and Yang, Guang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2410.13823}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IJCAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Cyclic_Vision_LanguageManipulator.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Cyclic_Vision_LanguageManipulator.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Fang2024CyclicVM" class="col-sm-8"> <div class="title">Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation</div> <div class="author"> Yingying Fang, Zihao Jin, Shaojie Guo, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Jinda Liu, Zhiling Yue, Yijian Gao, Junzhi Ning, Zhi Li, Simon L. F. Walsh, Guang Yang' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>In International Joint Conference on Artificial Intelligence</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Despite significant advancements in automated report generation, the opaqueness of text interpretability continues to cast doubt on the reliability of the content produced. This paper introduces a novel approach to identify specific image features in X-ray images that influence the outputs of report generation models. Specifically, we propose Cyclic Vision-Language Manipulator CVLM, a module to generate a manipulated X-ray from an original X-ray and its report from a designated report generator. The essence of CVLM is that cycling manipulated X-rays to the report generator produces altered reports aligned with the alterations pre-injected into the reports for X-ray generation, achieving the term "cyclic manipulation". This process allows direct comparison between original and manipulated X-rays, clarifying the critical image features driving changes in reports and enabling model users to assess the reliability of the generated texts. Empirical evaluations demonstrate that CVLM can identify more precise and reliable features compared to existing explanation methods, significantly enhancing the transparency and applicability of AI-generated reports.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Fang2024CyclicVM</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fang, Yingying and Jin, Zihao and Guo, Shaojie and Liu, Jinda and Yue, Zhiling and Gao, Yijian and Ning, Junzhi and Li, Zhi and Walsh, Simon L. F. and Yang, Guang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Joint Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://api.semanticscholar.org/CorpusID:273950041}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">WACV</abbr> <figure> <picture> <img src="/assets/img/publication_preview/DMRN.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="DMRN.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2025wacv" class="col-sm-8"> <div class="title">DMRN: A Dynamical Multi-Order Response Network for the Robust Lung Airway Segmentation</div> <div class="author"> Sheng Zhang, Jinge Wu, <em>Junzhi Ning</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Guang Yang' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>WACV</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Zhang_DMRN_A_Dynamical_Multi-Order_Response_Network_for_the_Robust_Lung_WACV_2025_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Automated airway segmentation in CT images is crucial for lung diseases’ diagnosis. However, manual annotation scarcity hinders supervised learning efficacy, while unlimited intensities and sample imbalance lead to discontinuity and false-negative issues. To address these challenges, we propose a novel airway segmentation model named Dynamical Multi-order Response Network (DMRN), integrating the unsupervised and supervised learning in parallel to alleviate the label scarcity of airway. In the unsupervised branch, (1) we propose several novel strategies of Dynamic Mask-Ratio (DMR) to enable the model to perceive context information of varying sizes, mimicking the laws of human learning vividly; (2) we present a novel target of Multi-Order Normalized Responses (MONR), exploiting the distinct order exponential operation of raw images and oriented gradients to enhance the textural representations of bronchioles. For the supervised branch, we directly predict the final full segmentation map by the large-ratio cube-masked input instead of full input. Ultimately, we verify the method performance and robustness by training on normal lung disease datasets, while testing on lung cancer, COVID-19 and Lung fibrosis datasets. All experimental results have proved that our method exceeds state-of-the-art methods significantly.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2025wacv</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DMRN: A Dynamical Multi-Order Response Network for the Robust Lung Airway Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Sheng and Wu, Jinge and Ning, Junzhi and Yang, Guang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{WACV}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Winter Conference on Applications of Computer Vision}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4036-4045}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/GMAI_R1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="GMAI_R1.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="su2025gmai" class="col-sm-8"> <div class="title">GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning</div> <div class="author"> Yanzhou Su, Tianbin Li, Jiyao Liu, and <span class="more-authors" title="click to view 15 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '15 more authors' ? 'Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng Chen, Ming Hu, Shixiang Tang, Lihao Liu, Bin Fu, Wenqi Shao, Xiaowei Hu, Xiangwen Liao, Yuanfeng Ji, Junjun He' : '15 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">15 more authors</span> </div> <div class="periodical"> <em>arXiv preprint</em>, 2025 </div> <div class="periodical"> Under Review </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2504.01886" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Recent advances in general medical AI have made significant strides, but existing models often lack the reasoning capabilities needed for complex medical decision-making. This paper presents GMAI-VL-R1, a multimodal medical reasoning model enhanced by reinforcement learning (RL) to improve its reasoning abilities. Through iterative training, GMAI-VL-R1 optimizes decision-making, significantly boosting diagnostic accuracy and clinical support. We also develop a reasoning data synthesis method, generating step-by-step reasoning data via rejection sampling, which further enhances the model’s generalization. Experimental results show that after RL training, GMAI-VL-R1 excels in tasks such as medical image diagnosis and visual question answering. While the model demonstrates basic memorization with supervised fine-tuning, RL is crucial for true generalization. Our work establishes new evaluation benchmarks and paves the way for future advancements in medical reasoning models. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">su2025gmai</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Su, Yanzhou and Li, Tianbin and Liu, Jiyao and Ma, Chenglong and Ning, Junzhi and Tang, Cheng and Ju, Sibo and Ye, Jin and Chen, Pengcheng and Hu, Ming and Tang, Shixiang and Liu, Lihao and Fu, Bin and Shao, Wenqi and Hu, Xiaowei and Liao, Xiangwen and Ji, Yuanfeng and He, Junjun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2504.01886}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Under Review}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/surgical_ophora.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="surgical_ophora.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2025ophora" class="col-sm-8"> <div class="title">Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model</div> <div class="author"> Wei Li, Ming Hu, Guoan Wang, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>MICCAI</em>, 2025 </div> <div class="periodical"> Oral Presentation </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at https://github.com/uni-medical/Ophora.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">li2025ophora</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Wei and Hu, Ming and Wang, Guoan and Liu, Lihao and Zhou, Kaijin and Ning, Junzhi and Guo, Xin and Ge, Zongyuan and Gu, Lixu and He, Junjun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{MICCAI}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Medical Image Computing and Computer Assisted Intervention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Oral Presentation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/spatial_reward_r1_cxr.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="spatial_reward_r1_cxr.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2025medground" class="col-sm-8"> <div class="title">MedGround-R1: Advancing Medical Image Grounding via Spatial-Semantic Rewarded Group Relative Policy Optimization</div> <div class="author"> Huihui Xu, Yuanpeng Nie, Hualiang Wang, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Ying Chen, Wei Li, Junzhi Ning, Lihao Liu, Hongqiu Wang, Lei Zhu, Jiyao Liu, Xiaomeng Li, Junjun He' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em>MICCAI</em>, 2025 </div> <div class="periodical"> Spotlight </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Medical Image Grounding (MIG), which involves localizing specific regions in medical images based on textual descriptions, requires models to not only perceive regions but also deduce spatial relationships of these regions. Existing Vision-Language Models (VLMs) for MIG often rely on Supervised Fine-Tuning (SFT) with large amounts of Chain-of-Thought (CoT) reasoning annotations, which are expensive and time-consuming to acquire. Recently, DeepSeek-R1 demonstrated that Large Language Models (LLMs) can acquire reasoning abilities through Group Relative Policy Optimization (GRPO) without requiring CoT annotations. In this paper, we adapt the GRPO reinforcement learning framework to VLMs for Medical Image Grounding. We propose the Spatial-Semantic Rewarded Group Relative Policy Optimization to train the model without CoT reasoning annotations. Specifically, we introduce Spatial-Semantic Rewards, which combine spatial accuracy reward and semantic consistency reward to provide nuanced feedback for both spatially positive and negative completions. Additionally, we propose to use the Chain-of-Box template, which integrates visual information of referring bounding boxes into the reasoning process, enabling the model to explicitly reason about spatial regions during intermediate steps. Experiments on three datasets MS-CXR, ChestX-ray8, and M3D-RefSeg demonstrate that our method achieves state-of-the-art performance in Medical Image Grounding. Ablation studies further validate the effectiveness of each component in our approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xu2025medground</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MedGround-R1: Advancing Medical Image Grounding via Spatial-Semantic Rewarded Group Relative Policy Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Huihui and Nie, Yuanpeng and Wang, Hualiang and Chen, Ying and Li, Wei and Ning, Junzhi and Liu, Lihao and Wang, Hongqiu and Zhu, Lei and Liu, Jiyao and Li, Xiaomeng and He, Junjun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{MICCAI}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Medical Image Computing and Computer Assisted Intervention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Spotlight}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/multi-model_mri.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="multi-model_mri.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="multimodal2025miccai" class="col-sm-8"> <div class="title">Multi-modal MRI Translation via Evidential Regression and Distribution Calibration</div> <div class="author"> Jiyao Liu, Shangqi Gao, Yuxin Li, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Lihao Liu, Xin Gao, Zhaohu Xing, Junzhi Ning, Yanzhou Su, Xiao-Yong Zhang, Junjun He, Ningsheng Xu, Xiahai Zhuang' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em>MICCAI</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Multi-modal Magnetic Resonance Imaging (MRI) translation leverages information from source MRI sequences to generate target modalities, enabling comprehensive diagnosis while overcoming the limitations of acquiring all sequences. While existing deep-learning-based multi-modal MRI translation methods have shown promising potential, they still face two key challenges: 1) lack of reliable uncertainty quantification for synthesized images, and 2) limited robustness when deployed across different medical centers. To address these challenges, we propose a novel framework that reformulates multi-modal MRI translation as a multi-modal evidential regression problem with distribution calibration. Our approach incorporates two key components: 1) an evidential regression module that estimates uncertainties from different source modalities and an explicit distribution mixture strategy for transparent multi-modal fusion, and 2) a distribution calibration mechanism that adapts to source-target mapping shifts to ensure consistent performance across different medical centers. Extensive experiments on three datasets from the BraTS2023 challenge demonstrate that our framework achieves superior performance and robustness across domains.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">multimodal2025miccai</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-modal MRI Translation via Evidential Regression and Distribution Calibration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Jiyao and Gao, Shangqi and Li, Yuxin and Liu, Lihao and Gao, Xin and Xing, Zhaohu and Ning, Junzhi and Su, Yanzhou and Zhang, Xiao-Yong and He, Junjun and Xu, Ningsheng and Zhuang, Xiahai}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{MICCAI}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Medical Image Computing and Computer Assisted Intervention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Technical Report</abbr> <figure> <picture> <img src="/assets/img/publication_preview/text_lan_fusion.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="text_lan_fusion.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bai2025interns1" class="col-sm-8"> <div class="title">Intern-S1: A Scientific Multimodal Foundation Model</div> <div class="author"> Lei Bai, Zhongrui Cai, Yuhang Cao, and <span class="more-authors" title="click to view 97 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '97 more authors' ? 'Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqing Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qitan Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao' : '97 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">97 more authors</span> </div> <div class="periodical"> <em>arXiv preprint</em>, 2025 </div> <div class="periodical"> Under Review </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training. On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bai2025interns1</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Intern-S1: A Scientific Multimodal Foundation Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bai, Lei and Cai, Zhongrui and Cao, Yuhang and Cao, Maosong and Cao, Weihan and Chen, Chiyu and Chen, Haojiong and Chen, Kai and Chen, Pengcheng and Chen, Ying and Chen, Yongkang and Cheng, Yu and Chu, Pei and Chu, Tao and Cui, Erfei and Cui, Ganqu and Cui, Long and Cui, Ziyun and Deng, Nianchen and Ding, Ning and Dong, Nanqing and Dong, Peijie and Dou, Shihan and Du, Sinan and Duan, Haodong and Fan, Caihua and Gao, Ben and Gao, Changjiang and Gao, Jianfei and Gao, Songyang and Gao, Yang and Gao, Zhangwei and Ge, Jiaye and Ge, Qiming and Gu, Lixin and Gu, Yuzhe and Guo, Aijia and Guo, Qipeng and Guo, Xu and He, Conghui and He, Junjun and Hong, Yili and Hou, Siyuan and Hu, Caiyu and Hu, Hanglei and Hu, Jucheng and Hu, Ming and Hua, Zhouqi and Huang, Haian and Huang, Junhao and Huang, Xu and Huang, Zixian and Jiang, Zhe and Kong, Lingkai and Li, Linyang and Li, Peiji and Li, Pengze and Li, Shuaibin and Li, Tianbin and Li, Wei and Li, Yuqiang and Lin, Dahua and Lin, Junyao and Lin, Tianyi and Lin, Zhishan and Liu, Hongwei and Liu, Jiangning and Liu, Jiyao and Liu, Junnan and Liu, Kai and Liu, Kaiwen and Liu, Kuikun and Liu, Shichun and Liu, Shudong and Liu, Wei and Liu, Xinyao and Liu, Yuhong and Liu, Zhan and Lu, Yinquan and Lv, Haijun and Lv, Hongxia and Lv, Huijie and Lv, Qitan and Lv, Ying and Lyu, Chengqi and Ma, Chenglong and Ma, Jianpeng and Ma, Ren and Ma, Runmin and Ma, Runyuan and Ma, Xinzhu and Ma, Yichuan and Ma, Zihan and Mi, Sixuan and Ning, Junzhi and Ning, Wenchang and Pang, Xinle and Peng, Jiahui and Peng, Runyu and Qiao, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2508.15763}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Under Review}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/scientific_llm_survey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="scientific_llm_survey.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hu2025survey" class="col-sm-8"> <div class="title">A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers</div> <div class="author"> Ming Hu, Chenglong Ma, Wei Li, and <span class="more-authors" title="click to view 97 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '97 more authors' ? 'Wanghan Xu, Jiamin Wu, Jucheng Hu, Tianbin Li, Guohang Zhuang, Jiaqi Liu, Yingzhou Lu, Ying Chen, Chaoyang Zhang, Cheng Tan, Jie Ying, Guocheng Wu, Shujian Gao, Pengcheng Chen, Jiashi Lin, Haitao Wu, Lulu Chen, Fengxiang Wang, Yuanyuan Zhang, Xiangyu Zhao, Feilong Tang, Encheng Su, Junzhi Ning, Xinyao Liu, Ye Du, Changkai Ji, Cheng Tang, Huihui Xu, Ziyang Chen, Ziyan Huang, Jiyao Liu, Pengfei Jiang, Yizhou Wang, Chen Tang, Jianyu Wu, Yuchen Ren, Siyuan Yan, Zhonghua Wang, Zhongxing Xu, Shiyan Su, Shangquan Sun, Runkai Zhao, Zhisheng Zhang, Yu Liu, Fudi Wang, Yuanfeng Ji, Yanzhou Su, Hongming Shan, Chun-Mei Feng, Jiahao Xu, Jiangtao Yan, Wenhao Tang, Diping Song, Lihao Liu, Yanyan Huang, Lequan Yu, Bin Fu, Shujun Wang, Xiaomeng Li, Xiaowei Hu, Yun Gu, Ben Fei, Zhongying Deng, Benyou Wang, Yuewen Cao, Minjie Shen, Haodong Duan, Jie Xu, Yirong Chen, Fang Yan, Hongxia Hao, Jielan Li, Jiajun Du, Yanbo Wang, Imran Razzak, Chi Zhang, Lijun Wu, Conghui He, Zhaohui Lu, Jinhai Huang, Yihao Liu, Fenghua Ling, Yuqiang Li, Aoran Wang, Qihao Zheng, Nanqing Dong, Tianfan Fu, Dongzhan Zhou, Yan Lu, Wenlong Zhang, Jin Ye, Jianfei Cai, Wanli Ouyang, Yu Qiao, Zongyuan Ge, Shixiang Tang, Junjun He' : '97 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">97 more authors</span> </div> <div class="periodical"> <em>arXiv preprint</em>, 2025 </div> <div class="periodical"> Under Review </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2508.21148" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands – heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2025survey</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Ming and Ma, Chenglong and Li, Wei and Xu, Wanghan and Wu, Jiamin and Hu, Jucheng and Li, Tianbin and Zhuang, Guohang and Liu, Jiaqi and Lu, Yingzhou and Chen, Ying and Zhang, Chaoyang and Tan, Cheng and Ying, Jie and Wu, Guocheng and Gao, Shujian and Chen, Pengcheng and Lin, Jiashi and Wu, Haitao and Chen, Lulu and Wang, Fengxiang and Zhang, Yuanyuan and Zhao, Xiangyu and Tang, Feilong and Su, Encheng and Ning, Junzhi and Liu, Xinyao and Du, Ye and Ji, Changkai and Tang, Cheng and Xu, Huihui and Chen, Ziyang and Huang, Ziyan and Liu, Jiyao and Jiang, Pengfei and Wang, Yizhou and Tang, Chen and Wu, Jianyu and Ren, Yuchen and Yan, Siyuan and Wang, Zhonghua and Xu, Zhongxing and Su, Shiyan and Sun, Shangquan and Zhao, Runkai and Zhang, Zhisheng and Liu, Yu and Wang, Fudi and Ji, Yuanfeng and Su, Yanzhou and Shan, Hongming and Feng, Chun-Mei and Xu, Jiahao and Yan, Jiangtao and Tang, Wenhao and Song, Diping and Liu, Lihao and Huang, Yanyan and Yu, Lequan and Fu, Bin and Wang, Shujun and Li, Xiaomeng and Hu, Xiaowei and Gu, Yun and Fei, Ben and Deng, Zhongying and Wang, Benyou and Cao, Yuewen and Shen, Minjie and Duan, Haodong and Xu, Jie and Chen, Yirong and Yan, Fang and Hao, Hongxia and Li, Jielan and Du, Jiajun and Wang, Yanbo and Razzak, Imran and Zhang, Chi and Wu, Lijun and He, Conghui and Lu, Zhaohui and Huang, Jinhai and Liu, Yihao and Ling, Fenghua and Li, Yuqiang and Wang, Aoran and Zheng, Qihao and Dong, Nanqing and Fu, Tianfan and Zhou, Dongzhan and Lu, Yan and Zhang, Wenlong and Ye, Jin and Cai, Jianfei and Ouyang, Wanli and Qiao, Yu and Ge, Zongyuan and Tang, Shixiang and He, Junjun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2508.21148}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Under Review}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6E%69%6E%67%6A%75%6E%7A%68%69%38%35@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/junzhin" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/junzhin" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://www.semanticscholar.org/author/2353285720" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://openreview.net/profile?id=%7EJunzhi_Ning1" title="OpenReview" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/openreview.jpeg" alt="OpenReview"> </a> </div> <div class="contact-note">Feel free to contact me for research opportunities and collaboration. </div> </div> <h2> <a style="color: inherit">🙏 Acknowledgments</a> </h2> <div class="acknowledgments"> <p>I am deeply grateful to my supervisors <a href="https://scholar.google.com/citations?user=Z4LgebkAAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank"><strong>Dr. Junjun He</strong></a>, <a href="https://scholar.google.com/citations?user=ZfzEFpsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><strong>Dr. Guang Yang</strong></a>, <a href="https://scholar.google.com/citations?user=xpAYtroAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><strong>Dr. Matthieu Komorowski</strong></a>, and <strong>Prof. Minming Gong</strong> for their invaluable guidance, mentorship, and support throughout my research journey. Their insights and encouragement have been instrumental in shaping my academic growth.</p> <p>I also extend my sincere thanks to my collaborators <strong>Lihao Liu</strong>, <strong>Sheng Zhang</strong>, <strong>Xiaodan Xing</strong>, <strong>Yingying Fang</strong>, <strong>Cheng Tang</strong>, <strong>Wei Li</strong>, <strong>Jiyao Liu</strong>, <strong>Huihui Xu</strong>, and many others. Their expertise, dedication, and teamwork have been essential to our research achievements.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Junzhi (Raymond) Ning. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>