<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="robots" content="noindex, nofollow, noarchive, nosnippet"> <title> Junzhi (Raymond) Ning </title> <meta name="author" content="Junzhi (Raymond) Ning"> <meta name="description" content="Portrait of a Man in Quest of the unknown, yet satisfied. MRes in Machine Learning at Imperial College London. "> <meta name="keywords" content="machine-learning, medical-imaging, artificial-intelligence, computer-vision, deep-learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/optimized_solution.jpg?ca2ddf899b29608d36c5260cf0b4a94a"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://junzhin.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Junzhi</span> (Raymond) Ning </h1> <p class="desc">Machine Learning Researcher at Shanghai AI Lab</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/optimized_solution-480.webp 480w,/assets/img/optimized_solution-800.webp 800w,/assets/img/optimized_solution-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/optimized_solution.jpg?ca2ddf899b29608d36c5260cf0b4a94a" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="optimized_solution.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>Machine Learning Researcher</p> <p>Shanghai AI Lab, Shanghai, China</p> <p>Supervised by Dr. Junjun He</p> <p>MRes Graduate from Imperial College London</p> </div> </div> <div class="clearfix"> <p>I am a Machine Learning Researcher at Shanghai AI Lab, where I conduct cutting-edge research on multimodal AI models with a focus on medical applications. Currently supervised by <strong>Dr. Junjun He</strong>, my work centers on developing advanced AI systems that bridge the gap between computer vision and healthcare applications.</p> <p>At Shanghai AI Lab, I conduct research on multimodal models with applications in general medical scenarios, focusing on designing workflows for large-scale data generation to create synthetic datasets with millions of samples. I have led the development of <strong>RetinaLogos</strong>, a groundbreaking text-guided retinal image generation model that synthesizes high-resolution color fundus photography (CFP) images with precise anatomical details. Additionally, I contribute to <strong>Project Imaging-X</strong>, a strategic initiative consolidating 1000+ open medical imaging datasets worldwide to build foundational infrastructure for next-generation medical AI models.</p> <p>During my time as a Graduate Research Student at Imperial College London (Oct 2023 - Oct 2024), I actively engaged in lab activities including hosting Journal Clubs and collaborating closely with medical and ICU clinicians. I participated in multiple research projects, managed paper submission workflows for top conferences like MICCAI, and contributed to securing industrial funding for innovative research proposals. I completed my MRes with Distinction (First-Class Honours) and achieved <strong>Top 5 ranking</strong> among the entire MRes in AI and ML cohort, supervised by <a href="https://www.linkedin.com/in/matthieukomorowski/" rel="external nofollow noopener" target="_blank"><strong>Dr. Matthieu Komorowski</strong></a> and <a href="https://www.linkedin.com/in/gyangmedia/" rel="external nofollow noopener" target="_blank"><strong>Dr. Guang Yang</strong></a>.</p> <p>My educational background includes a Bachelor of Science (Honours) in Data Science from The University of Sydney (Overall WAM 89.5 with <strong>University Medal</strong>) and a Bachelor of Science in Mathematics and Statistics from The University of Melbourne (First-Class Honours).</p> <p>My current research focuses on advancing <strong>Generative AI</strong> for healthcare applications, with specific achievements in:</p> <ul> <li> <strong>Multimodal Medical AI</strong>: Developing GMAI-VL-R1, a reinforcement learning-enhanced model for complex medical reasoning and decision-making</li> <li> <strong>Large-scale Medical Data Synthesis</strong>: Creating the RetinaLogos-1400k dataset with 1.4 million synthetic retinal images for ophthalmology applications</li> <li> <strong>Vision-Language Models in Healthcare</strong>: Pioneering text-guided medical image generation and analysis systems</li> <li> <strong>Deep Generative Models</strong>: Advancing medical image translation, particularly in chest X-ray opacity removal and anatomical structure enhancement</li> <li> <strong>Synthetic Dataset Creation</strong>: Designing scalable workflows for generating millions of high-quality medical training samples</li> </ul> <p>My research has resulted in publications at top-tier venues including <strong>MICCAI 2025</strong> (4 accepted papers, including 1 oral presentation and 1 spotlight), <strong>ISBI 2025</strong>, <strong>WACV 2025</strong>, <strong>IJCAI 2025</strong>, <strong>Pattern Recognition Letters</strong>, and <strong>NeurIPS Workshop</strong>. I have submissions under review at <strong>AAAI 2026</strong>. Key achievements include developing the largest synthetic retinal image dataset (RetinaLogos-1400k with 1.4M images), contributing to Project Imaging-X consolidating 1000+ medical datasets, and advancing reinforcement learning approaches for medical reasoning.</p> <p><strong>Awards and Recognition:</strong></p> <ul> <li> <strong>University Medal</strong> in Bachelor of Science (Honours), University of Sydney (2023)</li> <li> <strong>Melbourne International Undergraduate Scholarship</strong>, University of Melbourne (2022)</li> <li> <strong>Dean‚Äôs Honours List</strong>, University of Melbourne (2019)</li> </ul> <p><strong>I am actively seeking PhD positions for Fall 2025 and Spring 2026, as well as research internship opportunities.</strong></p> <p>Feel free to contact me for research opportunities and collaboration.</p> <p><em>‚ÄúPositivity is the essence of progress. In every challenge, I see an opportunity for learning and growth.‚Äù</em></p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 17, 2025</th> <td> üéâ <strong>First-author paper accepted at MICCAI 2025!</strong> RetinaLogos: A novel text-guided retinal image synthesis model with fine-grained anatomical control. üëÅÔ∏è </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 17, 2025</th> <td> üéâ <strong>4 papers accepted at MICCAI 2025!</strong> Including 1 first-author paper (RetinaLogos), 1 oral presentation (Ophora), and 1 spotlight (MedGround-R1). Proud to contribute to advancing medical AI research! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 28, 2025</th> <td> üìÑ <strong>Cyclic Vision-Language Manipulator</strong> paper accepted at <strong>IJCAI 2025</strong>! Joint work on reliable image interpretation for automated report generation in medical imaging. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 15, 2024</th> <td> üéâ <strong>First-author paper accepted at ISBI 2025</strong>! ‚ÄúUnveiling the Capabilities of Latent Diffusion Models for Classification of Lung Diseases in Chest X-Rays‚Äù explores new frontiers in medical AI. </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 01, 2024</th> <td> üöÄ Started as Machine Learning Researcher at <strong>Shanghai AI Lab</strong>, focusing on multimodal medical AI models and large-scale synthetic dataset generation for healthcare applications. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/retina_logos.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="retina_logos.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ning2025retinalogos" class="col-sm-8"> <div class="title">RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions</div> <div class="author"> <em>Junzhi Ning</em>, Cheng Tang, Kaijing Zhou, and <span class="more-authors" title="click to view 12 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '12 more authors' ? 'Diping Song, Lihao Liu, Ming Hu, Wei Li, Huihui Xu, Yanzhou Su, Tianbin Li, Jiyao Liu, Jin Ye, Sheng Zhang, Yuanfeng Ji, Junjun He' : '12 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">12 more authors</span> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://anonymous.4open.science/r/Text-Driven-CFP-Generator" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The scarcity of high-quality, labelled retinal imaging data presents a significant challenge in the development of machine learning models for ophthalmology. To address this, we introduce RetinaLogos-1400k, a large-scale synthetic Caption-CFP dataset with 1.4 million entries.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ning2025retinalogos</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ning, Junzhi and Tang, Cheng and Zhou, Kaijing and Song, Diping and Liu, Lihao and Hu, Ming and Li, Wei and Xu, Huihui and Su, Yanzhou and Li, Tianbin and Liu, Jiyao and Ye, Jin and Zhang, Sheng and Ji, Yuanfeng and He, Junjun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Medical Image Computing and Computer Assisted Intervention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ISBI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/latent_diffusion_cxr.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="latent_diffusion_cxr.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ning2025latent" class="col-sm-8"> <div class="title">Unveiling the Capabilities of Latent Diffusion Models for Classification of Lung Diseases in Chest X-Rays</div> <div class="author"> <em>Junzhi Ning</em>, Xiaodan Xing, Sheng Zhang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Xiao Ma, Guang Yang' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2411.XXXXX" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Diffusion models have demonstrated a remarkable ability to synthesize Chest X-Ray (CXR) images, particularly by generating high-quality samples to address the scarcity and imbalance of annotated CXRs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ning2025latent</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unveiling the Capabilities of Latent Diffusion Models for Classification of Lung Diseases in Chest X-Rays}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ning, Junzhi and Xing, Xiaodan and Zhang, Sheng and Ma, Xiao and Yang, Guang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Symposium on Biomedical Imaging}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-5}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">PRL</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cxr_lung_opacity.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cxr_lung_opacity.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ning2025cxr" class="col-sm-8"> <div class="title">Unpaired translation of chest X-ray images for lung opacity diagnosis via adaptive activation masks and cross-domain alignment</div> <div class="author"> <em>Junzhi Ning</em>, Dominic C. Marshall, Yijian Gao, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Xiaodan Xing, Yang Nan, Yingying Fang, Sheng Zhang, Matthieu Komorowski, Guang Yang' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>Pattern Recognition Letters</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2410.XXXXX" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>This project addresses the challenge of diagnosing cardiopulmonary diseases in chest X-rays (CXRs) when lung opacities obscure critical anatomical details.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ning2025cxr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unpaired translation of chest X-ray images for lung opacity diagnosis via adaptive activation masks and cross-domain alignment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ning, Junzhi and Marshall, Dominic C. and Gao, Yijian and Xing, Xiaodan and Nan, Yang and Fang, Yingying and Zhang, Sheng and Komorowski, Matthieu and Yang, Guang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Pattern Recognition Letters}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{193}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{21-28}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">WACV</abbr> <figure> <picture> <img src="/assets/img/publication_preview/DMRN.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="DMRN.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2025wacv" class="col-sm-8"> <div class="title">DMRN: A Dynamical Multi-Order Response Network for the Robust Lung Airway Segmentation</div> <div class="author"> Sheng Zhang, Jinge Wu, <em>Junzhi Ning</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Guang Yang' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Zhang_DMRN_A_Dynamical_Multi-Order_Response_Network_for_the_Robust_Lung_WACV_2025_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>DMRN introduces a dynamical multiorder response network for robust lung airway segmentation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2025wacv</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DMRN: A Dynamical Multi-Order Response Network for the Robust Lung Airway Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Sheng and Wu, Jinge and Ning, Junzhi and Yang, Guang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Winter Conference on Applications of Computer Vision}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4036-4045}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/mask2ct.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mask2ct.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xing2024dgm" class="col-sm-8"> <div class="title">Deep Generative Models Unveil Patterns in Medical Images Through Vision-Language Conditioning</div> <div class="author"> Xiaodan Xing, <em>Junzhi Ning</em>, Yang Nan, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Guang Yang' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>CoRR</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2410.13823" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/junzhin/DGM-VLC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This project explores how deep generative models can go beyond traditional data augmentation in medical imaging by uncovering and demonstrating clinical patterns within medical images.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xing2024dgm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Generative Models Unveil Patterns in Medical Images Through Vision-Language Conditioning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xing, Xiaodan and Ning, Junzhi and Nan, Yang and Yang, Guang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{CoRR}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2410.13823}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/GMAI_R1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="GMAI_R1.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="su2025gmai" class="col-sm-8"> <div class="title">GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning</div> <div class="author"> Yanzhou Su, Tianbin Li, Jiyao Liu, and <span class="more-authors" title="click to view 15 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '15 more authors' ? 'Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng Chen, Ming Hu, Shixiang Tang, Lihao Liu, Bin Fu, Wenqi Shao, Xiaowei Hu, Xiangwen Liao, Yuanfeng Ji, Junjun He' : '15 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">15 more authors</span> </div> <div class="periodical"> <em>CoRR</em>, 2025 </div> <div class="periodical"> Under Review </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2504.01886" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Recent advances in general medical AI have made significant strides, but existing models often lack the reasoning capabilities needed for complex medical decision-making. This paper presents GMAI-VL-R1, a multimodal medical reasoning model enhanced by reinforcement learning (RL) to improve its reasoning abilities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">su2025gmai</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Su, Yanzhou and Li, Tianbin and Liu, Jiyao and Ma, Chenglong and Ning, Junzhi and Tang, Cheng and Ju, Sibo and Ye, Jin and Chen, Pengcheng and Hu, Ming and Tang, Shixiang and Liu, Lihao and Fu, Bin and Shao, Wenqi and Hu, Xiaowei and Liao, Xiangwen and Ji, Yuanfeng and He, Junjun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{CoRR}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2504.01886}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Under Review}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/surgical_ophora.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="surgical_ophora.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2025ophora" class="col-sm-8"> <div class="title">Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model</div> <div class="author"> Wei Li, Ming Hu, Guoan Wang, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> Oral Presentation </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Ophora presents a large-scale data-driven approach for text-guided ophthalmic surgical video generation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">li2025ophora</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Wei and Hu, Ming and Wang, Guoan and Liu, Lihao and Zhou, Kaijin and Ning, Junzhi and Guo, Xin and Ge, Zongyuan and Gu, Lixu and He, Junjun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Medical Image Computing and Computer Assisted Intervention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Oral Presentation}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/spatial_reward_r1_cxr.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="spatial_reward_r1_cxr.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2025medground" class="col-sm-8"> <div class="title">MedGround-R1: Advancing Medical Image Grounding via Spatial-Semantic Rewarded Group Relative Policy Optimization</div> <div class="author"> Huihui Xu, Yuanpeng Nie, Hualiang Wang, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Ying Chen, Wei Li, Junzhi Ning, Lihao Liu, Hongqiu Wang, Lei Zhu, Jiyao Liu, Xiaomeng Li, Junjun He' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> Spotlight </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This work advances medical image grounding through spatial-semantic rewarded group relative policy optimization.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xu2025medground</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MedGround-R1: Advancing Medical Image Grounding via Spatial-Semantic Rewarded Group Relative Policy Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Huihui and Nie, Yuanpeng and Wang, Hualiang and Chen, Ying and Li, Wei and Ning, Junzhi and Liu, Lihao and Wang, Hongqiu and Zhu, Lei and Liu, Jiyao and Li, Xiaomeng and He, Junjun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Medical Image Computing and Computer Assisted Intervention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Spotlight}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/multi-model_mri.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="multi-model_mri.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="multimodal2025miccai" class="col-sm-8"> <div class="title">Multi-modal MRI Translation via Evidential Regression and Distribution Calibration</div> <div class="author"> Others and <em>Junzhi Ning</em> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>A novel approach for multi-modal MRI translation using evidential regression and distribution calibration.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">multimodal2025miccai</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-modal MRI Translation via Evidential Regression and Distribution Calibration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Others and Ning, Junzhi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Medical Image Computing and Computer Assisted Intervention}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IJCAI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Cyclic_Vision_LanguageManipulator.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Cyclic_Vision_LanguageManipulator.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="fang2025ijcaj" class="col-sm-8"> <div class="title">Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation</div> <div class="author"> Yingying Fang, Zihao Jin, Shaojie Guo, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Jinda Liu, Zhiling Yue, Yijian Gao, Junzhi Ning, Zhi Li, Simon Walsh, Guang Yang' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>A cyclic vision-language adapter approach for generating counterfactual explanations in report generation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">fang2025ijcaj</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fang, Yingying and Jin, Zihao and Guo, Shaojie and Liu, Jinda and Yue, Zhiling and Gao, Yijian and Ning, Junzhi and Li, Zhi and Walsh, Simon and Yang, Guang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Joint Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{357-366}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IJCAI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/meditok.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="meditok.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ma2025meditok" class="col-sm-8"> <div class="title">MedITok: A Unified Tokenizer for Medical Image Synthesis and Interpretation</div> <div class="author"> Chenglong Ma, Yuanfeng Ji, Jin Ye, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Zilong Li, Chenhui Wang, Junzhi Ning, Wei Li, Lihao Liu, Qiushan Guo, Tianbin Li, Junjun He, Hongming Shan' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em>CoRR</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2505.19225" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>MedITok presents a unified tokenizer for medical image synthesis and interpretation applications.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ma2025meditok</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MedITok: A Unified Tokenizer for Medical Image Synthesis and Interpretation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Chenglong and Ji, Yuanfeng and Ye, Jin and Li, Zilong and Wang, Chenhui and Ning, Junzhi and Li, Wei and Liu, Lihao and Guo, Qiushan and Li, Tianbin and He, Junjun and Shan, Hongming}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{CoRR}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2505.19225}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/text_lan_fusion.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="text_lan_fusion.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bai2025interns1" class="col-sm-8"> <div class="title">Intern-S1: A Scientific Multimodal Foundation Model</div> <div class="author"> Lei Bai, Zhongrui Cai, Yuhang Cao, and <span class="more-authors" title="click to view 97 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '97 more authors' ? 'Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqing Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qitan Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao' : '97 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">97 more authors</span> </div> <div class="periodical"> <em>CoRR</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Intern-S1 presents a scientific multimodal foundation model for various research applications.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bai2025interns1</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Intern-S1: A Scientific Multimodal Foundation Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bai, Lei and Cai, Zhongrui and Cao, Yuhang and Cao, Maosong and Cao, Weihan and Chen, Chiyu and Chen, Haojiong and Chen, Kai and Chen, Pengcheng and Chen, Ying and Chen, Yongkang and Cheng, Yu and Chu, Pei and Chu, Tao and Cui, Erfei and Cui, Ganqu and Cui, Long and Cui, Ziyun and Deng, Nianchen and Ding, Ning and Dong, Nanqing and Dong, Peijie and Dou, Shihan and Du, Sinan and Duan, Haodong and Fan, Caihua and Gao, Ben and Gao, Changjiang and Gao, Jianfei and Gao, Songyang and Gao, Yang and Gao, Zhangwei and Ge, Jiaye and Ge, Qiming and Gu, Lixin and Gu, Yuzhe and Guo, Aijia and Guo, Qipeng and Guo, Xu and He, Conghui and He, Junjun and Hong, Yili and Hou, Siyuan and Hu, Caiyu and Hu, Hanglei and Hu, Jucheng and Hu, Ming and Hua, Zhouqi and Huang, Haian and Huang, Junhao and Huang, Xu and Huang, Zixian and Jiang, Zhe and Kong, Lingkai and Li, Linyang and Li, Peiji and Li, Pengze and Li, Shuaibin and Li, Tianbin and Li, Wei and Li, Yuqiang and Lin, Dahua and Lin, Junyao and Lin, Tianyi and Lin, Zhishan and Liu, Hongwei and Liu, Jiangning and Liu, Jiyao and Liu, Junnan and Liu, Kai and Liu, Kaiwen and Liu, Kuikun and Liu, Shichun and Liu, Shudong and Liu, Wei and Liu, Xinyao and Liu, Yuhong and Liu, Zhan and Lu, Yinquan and Lv, Haijun and Lv, Hongxia and Lv, Huijie and Lv, Qitan and Lv, Ying and Lyu, Chengqi and Ma, Chenglong and Ma, Jianpeng and Ma, Ren and Ma, Runmin and Ma, Runyuan and Ma, Xinzhu and Ma, Yichuan and Ma, Zihan and Mi, Sixuan and Ning, Junzhi and Ning, Wenchang and Pang, Xinle and Peng, Jiahui and Peng, Runyu and Qiao, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{CoRR}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2508.15763}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/scientific_llm_survey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="scientific_llm_survey.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hu2025survey" class="col-sm-8"> <div class="title">A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers</div> <div class="author"> Ming Hu, Chenglong Ma, Wei Li, and <span class="more-authors" title="click to view 97 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '97 more authors' ? 'Wanghan Xu, Jiamin Wu, Jucheng Hu, Tianbin Li, Guohang Zhuang, Jiaqi Liu, Yingzhou Lu, Ying Chen, Chaoyang Zhang, Cheng Tan, Jie Ying, Guocheng Wu, Shujian Gao, Pengcheng Chen, Jiashi Lin, Haitao Wu, Lulu Chen, Fengxiang Wang, Yuanyuan Zhang, Xiangyu Zhao, Feilong Tang, Encheng Su, Junzhi Ning, Xinyao Liu, Ye Du, Changkai Ji, Cheng Tang, Huihui Xu, Ziyang Chen, Ziyan Huang, Jiyao Liu, Pengfei Jiang, Yizhou Wang, Chen Tang, Jianyu Wu, Yuchen Ren, Siyuan Yan, Zhonghua Wang, Zhongxing Xu, Shiyan Su, Shangquan Sun, Runkai Zhao, Zhisheng Zhang, Yu Liu, Fudi Wang, Yuanfeng Ji, Yanzhou Su, Hongming Shan, Chun-Mei Feng, Jiahao Xu, Jiangtao Yan, Wenhao Tang, Diping Song, Lihao Liu, Yanyan Huang, Lequan Yu, Bin Fu, Shujun Wang, Xiaomeng Li, Xiaowei Hu, Yun Gu, Ben Fei, Zhongying Deng, Benyou Wang, Yuewen Cao, Minjie Shen, Haodong Duan, Jie Xu, Yirong Chen, Fang Yan, Hongxia Hao, Jielan Li, Jiajun Du, Yanbo Wang, Imran Razzak, Chi Zhang, Lijun Wu, Conghui He, Zhaohui Lu, Jinhai Huang, Yihao Liu, Fenghua Ling, Yuqiang Li, Aoran Wang, Qihao Zheng, Nanqing Dong, Tianfan Fu, Dongzhan Zhou, Yan Lu, Wenlong Zhang, Jin Ye, Jianfei Cai, Wanli Ouyang, Yu Qiao, Zongyuan Ge, Shixiang Tang, Junjun He' : '97 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">97 more authors</span> </div> <div class="periodical"> <em>CoRR</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2508.21148" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>A comprehensive survey of scientific large language models examining their development from data foundations to advanced agent applications across various scientific domains.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2025survey</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Ming and Ma, Chenglong and Li, Wei and Xu, Wanghan and Wu, Jiamin and Hu, Jucheng and Li, Tianbin and Zhuang, Guohang and Liu, Jiaqi and Lu, Yingzhou and Chen, Ying and Zhang, Chaoyang and Tan, Cheng and Ying, Jie and Wu, Guocheng and Gao, Shujian and Chen, Pengcheng and Lin, Jiashi and Wu, Haitao and Chen, Lulu and Wang, Fengxiang and Zhang, Yuanyuan and Zhao, Xiangyu and Tang, Feilong and Su, Encheng and Ning, Junzhi and Liu, Xinyao and Du, Ye and Ji, Changkai and Tang, Cheng and Xu, Huihui and Chen, Ziyang and Huang, Ziyan and Liu, Jiyao and Jiang, Pengfei and Wang, Yizhou and Tang, Chen and Wu, Jianyu and Ren, Yuchen and Yan, Siyuan and Wang, Zhonghua and Xu, Zhongxing and Su, Shiyan and Sun, Shangquan and Zhao, Runkai and Zhang, Zhisheng and Liu, Yu and Wang, Fudi and Ji, Yuanfeng and Su, Yanzhou and Shan, Hongming and Feng, Chun-Mei and Xu, Jiahao and Yan, Jiangtao and Tang, Wenhao and Song, Diping and Liu, Lihao and Huang, Yanyan and Yu, Lequan and Fu, Bin and Wang, Shujun and Li, Xiaomeng and Hu, Xiaowei and Gu, Yun and Fei, Ben and Deng, Zhongying and Wang, Benyou and Cao, Yuewen and Shen, Minjie and Duan, Haodong and Xu, Jie and Chen, Yirong and Yan, Fang and Hao, Hongxia and Li, Jielan and Du, Jiajun and Wang, Yanbo and Razzak, Imran and Zhang, Chi and Wu, Lijun and He, Conghui and Lu, Zhaohui and Huang, Jinhai and Liu, Yihao and Ling, Fenghua and Li, Yuqiang and Wang, Aoran and Zheng, Qihao and Dong, Nanqing and Fu, Tianfan and Zhou, Dongzhan and Lu, Yan and Zhang, Wenlong and Ye, Jin and Cai, Jianfei and Ouyang, Wanli and Qiao, Yu and Ge, Zongyuan and Tang, Shixiang and He, Junjun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{CoRR}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2508.21148}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6E%69%6E%67%6A%75%6E%7A%68%69%38%35@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/junzhin" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://www.semanticscholar.org/author/2353285720" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://openreview.net/profile?id=%7EJunzhi_Ning1" title="OpenReview" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/openreview.jpeg" alt="OpenReview"> </a> </div> <div class="contact-note">Feel free to contact me for research opportunities and collaboration. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Junzhi (Raymond) Ning. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>