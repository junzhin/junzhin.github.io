---
---

@string{aps = {American Physical Society,}}
@string{neurips = {Neural Information Processing Systems,}}
@string{miccai = {Medical Image Computing and Computer Assisted Intervention,}}
@string{isbi = {IEEE International Symposium on Biomedical Imaging,}}
@string{tpami = {IEEE Transactions on Pattern Analysis and Machine Intelligence,}}
@string{wacv = {Winter Conference on Applications of Computer Vision,}}
@string{aaai = {Association for the Advancement of Artificial Intelligence,}}
@string{ijcai = {International Joint Conference on Artificial Intelligence,}}

@article{ning2025retinalogos,
  abbr={MICCAI},
  title={RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions},
  author={Ning, Junzhi and Tang, Cheng and Zhou, Kaijing and Song, Diping and Liu, Lihao and Hu, Ming and Li, Wei and Xu, Huihui and Su, Yanzhou and Li, Tianbin and Liu, Jiyao and Ye, Jin and Zhang, Sheng and Ji, Yuanfeng and He, Junjun},
  booktitle={Medical Image Computing and Computer Assisted Intervention},
  year={2025},
  publisher={Springer},
  preview={retina_logos.png},
  selected={true},
  bibtex_show={true},
  abstract={The scarcity of high-quality, labelled retinal imaging data presents a significant challenge in the development of machine learning models for ophthalmology. To address this, we introduce RetinaLogos-1400k, a large-scale synthetic Caption-CFP dataset with 1.4 million entries.},
  html={https://anonymous.4open.science/r/Text-Driven-CFP-Generator}
}

@article{ning2025latent,
  abbr={ISBI},
  title={Unveiling the Capabilities of Latent Diffusion Models for Classification of Lung Diseases in Chest X-Rays},
  author={Ning, Junzhi and Xing, Xiaodan and Zhang, Sheng and Ma, Xiao and Yang, Guang},
  booktitle={IEEE International Symposium on Biomedical Imaging},
  year={2025},
  pages={1-5},
  publisher={IEEE},
  preview={latent_diffusion_cxr.png},
  selected={true},
  bibtex_show={true},
  abstract={Diffusion models have demonstrated a remarkable ability to synthesize Chest X-Ray (CXR) images, particularly by generating high-quality samples to address the scarcity and imbalance of annotated CXRs.},
  html={https://arxiv.org/abs/2411.XXXXX}
}

@article{ning2025cxr,
  abbr={PRL},
  title={Unpaired translation of chest X-ray images for lung opacity diagnosis via adaptive activation masks and cross-domain alignment},
  author={Ning, Junzhi and Marshall, Dominic C. and Gao, Yijian and Xing, Xiaodan and Nan, Yang and Fang, Yingying and Zhang, Sheng and Komorowski, Matthieu and Yang, Guang},
  journal={Pattern Recognition Letters},
  year={2025},
  volume={193},
  pages={21-28},
  publisher={Elsevier},
  preview={cxr_lung_opacity.png},
  selected={true},
  bibtex_show={true},
  abstract={This project addresses the challenge of diagnosing cardiopulmonary diseases in chest X-rays (CXRs) when lung opacities obscure critical anatomical details.},
  html={https://arxiv.org/abs/2410.XXXXX}
}

@article{ning2023night2day,
  abbr={ADC},
  title={Enhancing Night-to-Day Image Translation with Semantic Prior and Reference Image Guidance},
  author={Ning, Junzhi and Gong, Mingming},
  booktitle={Australasian Database Conference},
  year={2023},
  pages={164-182},
  publisher={Springer},
  preview={semantic.png},
  selected={false},
  bibtex_show={true},
  abstract={This project introduces "RefN2D-Guide GAN," a novel method for improving night-to-day image translation by addressing the challenge of mapping images between domains with varying information richness.},
  html={https://link.springer.com/chapter/10.1007/978-3-031-47843-7_12}
}

@article{fang2025ijcai,
  abbr={IJCAI},
  title={Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation},
  author={Fang, Yingying and Jin, Zihao and Guo, Shaojie and Liu, Jinda and Yue, Zhiling and Gao, Yijian and Ning, Junzhi and Li, Zhi and Walsh, Simon and Yang, Guang},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2025},
  pages={357-366},
  publisher={IJCAI},
  preview={Cyclic_Vision_LanguageManipulator.png},
  selected={true},
  bibtex_show={true},
  abstract={A cyclic vision-language adapter approach for generating counterfactual explanations in report generation.}
}

@article{zhang2025wacv,
  abbr={WACV},
  title={DMRN: A Dynamical Multi-Order Response Network for the Robust Lung Airway Segmentation},
  author={Zhang, Sheng and Wu, Jinge and Ning, Junzhi and Yang, Guang},
  booktitle={Winter Conference on Applications of Computer Vision},
  year={2025},
  pages={4036-4045},
  publisher={IEEE},
  preview={DMRN.png},
  selected={true},
  bibtex_show={true},
  abstract={DMRN introduces a dynamical multiorder response network for robust lung airway segmentation.},
  pdf={https://openaccess.thecvf.com/content/WACV2025/papers/Zhang_DMRN_A_Dynamical_Multi-Order_Response_Network_for_the_Robust_Lung_WACV_2025_paper.pdf}
}

@article{wang2025tree,
  abbr={arXiv},
  title={THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?},
  author={Wang, Xin and Liu, Jiyao and Xiao, Yulong and Ning, Junzhi and Liu, Lihao and He, Junjun and Shi, Botian and Yu, Kaicheng},
  journal={CoRR},
  volume={abs/2506.21763},
  year={2025},
  publisher={arXiv},
  preview={the_tree.png},
  selected={true},
  bibtex_show={true},
  abstract={THE-Tree explores whether tracing historical evolution can enhance scientific verification and reasoning capabilities.},
  html={https://arxiv.org/abs/2506.21763}
}

@article{wei2025gui,
  abbr={arXiv},
  title={Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents},
  author={Wei, Jinjie and Liu, Jiyao and Liu, Lihao and Hu, Ming and Ning, Junzhi and Li, Mingcheng and Yin, Weijie and He, Junjun and Liang, Xiao and Feng, Chao and Yang, Dingkang},
  journal={CoRR},
  volume={abs/2506.17913},
  year={2025},
  publisher={arXiv},
  preview={gui_agents.png},
  selected={true},
  bibtex_show={true},
  abstract={A framework for Kahneman's dual-system intelligence in GUI agents through learning, reasoning, and refinement.},
  html={https://arxiv.org/abs/2506.17913}
}

@article{xing2024dgm,
  abbr={arXiv},
  title={Deep Generative Models Unveil Patterns in Medical Images Through Vision-Language Conditioning},
  author={Xing, Xiaodan and Ning, Junzhi and Nan, Yang and Yang, Guang},
  journal={CoRR},
  volume={abs/2410.13823},
  year={2024},
  publisher={arXiv},
  preview={mask2ct.png},
  selected={true},
  bibtex_show={true},
  abstract={This project explores how deep generative models can go beyond traditional data augmentation in medical imaging by uncovering and demonstrating clinical patterns within medical images.},
  html={http://arxiv.org/abs/2410.13823},
  code={https://github.com/junzhin/DGM-VLC}
}

@article{su2025gmai,
  abbr={arXiv},
  title={GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning},
  author={Su, Yanzhou and Li, Tianbin and Liu, Jiyao and Ma, Chenglong and Ning, Junzhi and Tang, Cheng and Ju, Sibo and Ye, Jin and Chen, Pengcheng and Hu, Ming and Tang, Shixiang and Liu, Lihao and Fu, Bin and Shao, Wenqi and Hu, Xiaowei and Liao, Xiangwen and Ji, Yuanfeng and He, Junjun},
  journal={CoRR},
  volume={abs/2504.01886},
  year={2025},
  publisher={arXiv},
  preview={GMAI_R1.png},
  selected={false},
  bibtex_show={true},
  note={Under Review},
  abstract={Recent advances in general medical AI have made significant strides, but existing models often lack the reasoning capabilities needed for complex medical decision-making. This paper presents GMAI-VL-R1, a multimodal medical reasoning model enhanced by reinforcement learning (RL) to improve its reasoning abilities.},
  html={https://arxiv.org/abs/2504.01886}
}

@article{li2025ophora,
  abbr={MICCAI},
  title={Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model},
  author={Li, Wei and Hu, Ming and Wang, Guoan and Liu, Lihao and Zhou, Kaijin and Ning, Junzhi and Guo, Xin and Ge, Zongyuan and Gu, Lixu and He, Junjun},
  booktitle={Medical Image Computing and Computer Assisted Intervention},
  year={2025},
  publisher={Springer},
  note={Oral Presentation},
  preview={surgical_ophora.png},
  selected={true},
  bibtex_show={true},
  abstract={Ophora presents a large-scale data-driven approach for text-guided ophthalmic surgical video generation.}
}

@article{xu2025medground,
  abbr={MICCAI},
  title={MedGround-R1: Advancing Medical Image Grounding via Spatial-Semantic Rewarded Group Relative Policy Optimization},
  author={Xu, Huihui and Nie, Yuanpeng and Wang, Hualiang and Chen, Ying and Li, Wei and Ning, Junzhi and Liu, Lihao and Wang, Hongqiu and Zhu, Lei and Liu, Jiyao and Li, Xiaomeng and He, Junjun},
  booktitle={Medical Image Computing and Computer Assisted Intervention},
  year={2025},
  publisher={Springer},
  note={Spotlight},
  preview={spatial_reward_r1_cxr.png},
  selected={false},
  bibtex_show={true},
  abstract={This work advances medical image grounding through spatial-semantic rewarded group relative policy optimization.}
}

@article{multimodal2025miccai,
  abbr={MICCAI},
  title={Multi-modal MRI Translation via Evidential Regression and Distribution Calibration},
  author={Others and Ning, Junzhi},
  booktitle={Medical Image Computing and Computer Assisted Intervention},
  year={2025},
  publisher={Springer},
  preview={multi-model_mri.png},
  selected={false},
  bibtex_show={true},
  abstract={A novel approach for multi-modal MRI translation using evidential regression and distribution calibration.}
}

@article{zhang2025wacv,
  abbr={WACV},
  title={DMRN: A Dynamical Multi-Order Response Network for the Robust Lung Airway Segmentation},
  author={Zhang, Sheng and Wu, Jinge and Ning, Junzhi and Yang, Guang},
  booktitle={Winter Conference on Applications of Computer Vision},
  year={2025},
  pages={4036-4045},
  publisher={IEEE},
  preview={DMRN.png},
  selected={true},
  bibtex_show={true},
  abstract={DMRN introduces a dynamical multiorder response network for robust lung airway segmentation.}
}

@article{ojemb2025anatomy,
  abbr={IEEE OJEMB},
  title={Anatomy-Guided Radiology Report Generation with Pathology-Aware Regional Prompts},
  author={Others and Ning, Junzhi},
  journal={IEEE Open Journal of Engineering in Medicine and Biology},
  year={2025},
  publisher={IEEE},
  preview={anatomy_guided_report.png},
  selected={false},
  bibtex_show={true},
  note={Under Review},
  abstract={A novel approach for anatomy-guided radiology report generation using pathology-aware regional prompts to improve diagnostic accuracy.}
}

@article{fang2025ijcai,
  abbr={IJCAI},
  title={Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation},
  author={Fang, Yingying and Jin, Zihao and Guo, Shaojie and Liu, Jinda and Yue, Zhiling and Gao, Yijian and Ning, Junzhi and Li, Zhi and Walsh, Simon and Yang, Guang},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2025},
  pages={357-366},
  publisher={IJCAI},
  preview={Cyclic_Vision_LanguageManipulator.png},
  selected={true},
  bibtex_show={true},
  abstract={A cyclic vision-language adapter approach for generating counterfactual explanations in report generation.}
}

@article{ma2025meditok,
  abbr={arXiv},
  title={MedITok: A Unified Tokenizer for Medical Image Synthesis and Interpretation},
  author={Ma, Chenglong and Ji, Yuanfeng and Ye, Jin and Li, Zilong and Wang, Chenhui and Ning, Junzhi and Li, Wei and Liu, Lihao and Guo, Qiushan and Li, Tianbin and He, Junjun and Shan, Hongming},
  journal={CoRR},
  volume={abs/2505.19225},
  year={2025},
  publisher={arXiv},
  preview={meditok.png},
  selected={false},
  bibtex_show={true},
  abstract={MedITok presents a unified tokenizer for medical image synthesis and interpretation applications.},
  html={https://arxiv.org/abs/2505.19225}
}

@article{wang2025tree,
  abbr={arXiv},
  title={THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?},
  author={Wang, Xin and Liu, Jiyao and Xiao, Yulong and Ning, Junzhi and Liu, Lihao and He, Junjun and Shi, Botian and Yu, Kaicheng},
  journal={CoRR},
  volume={abs/2506.21763},
  year={2025},
  publisher={arXiv},
  preview={the_tree.png},
  selected={true},
  bibtex_show={true},
  abstract={THE-Tree explores whether tracing historical evolution can enhance scientific verification and reasoning capabilities.}
}

@article{wei2025gui,
  abbr={arXiv},
  title={Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents},
  author={Wei, Jinjie and Liu, Jiyao and Liu, Lihao and Hu, Ming and Ning, Junzhi and Li, Mingcheng and Yin, Weijie and He, Junjun and Liang, Xiao and Feng, Chao and Yang, Dingkang},
  journal={CoRR},
  volume={abs/2506.17913},
  year={2025},
  publisher={arXiv},
  preview={gui_agents.png},
  selected={true},
  bibtex_show={true},
  abstract={A framework for Kahneman's dual-system intelligence in GUI agents through learning, reasoning, and refinement.}
}

@article{bai2025interns1,
  abbr={arXiv},
  title={Intern-S1: A Scientific Multimodal Foundation Model},
  author={Bai, Lei and Cai, Zhongrui and Cao, Yuhang and Cao, Maosong and Cao, Weihan and Chen, Chiyu and Chen, Haojiong and Chen, Kai and Chen, Pengcheng and Chen, Ying and Chen, Yongkang and Cheng, Yu and Chu, Pei and Chu, Tao and Cui, Erfei and Cui, Ganqu and Cui, Long and Cui, Ziyun and Deng, Nianchen and Ding, Ning and Dong, Nanqing and Dong, Peijie and Dou, Shihan and Du, Sinan and Duan, Haodong and Fan, Caihua and Gao, Ben and Gao, Changjiang and Gao, Jianfei and Gao, Songyang and Gao, Yang and Gao, Zhangwei and Ge, Jiaye and Ge, Qiming and Gu, Lixin and Gu, Yuzhe and Guo, Aijia and Guo, Qipeng and Guo, Xu and He, Conghui and He, Junjun and Hong, Yili and Hou, Siyuan and Hu, Caiyu and Hu, Hanglei and Hu, Jucheng and Hu, Ming and Hua, Zhouqi and Huang, Haian and Huang, Junhao and Huang, Xu and Huang, Zixian and Jiang, Zhe and Kong, Lingkai and Li, Linyang and Li, Peiji and Li, Pengze and Li, Shuaibin and Li, Tianbin and Li, Wei and Li, Yuqiang and Lin, Dahua and Lin, Junyao and Lin, Tianyi and Lin, Zhishan and Liu, Hongwei and Liu, Jiangning and Liu, Jiyao and Liu, Junnan and Liu, Kai and Liu, Kaiwen and Liu, Kuikun and Liu, Shichun and Liu, Shudong and Liu, Wei and Liu, Xinyao and Liu, Yuhong and Liu, Zhan and Lu, Yinquan and Lv, Haijun and Lv, Hongxia and Lv, Huijie and Lv, Qitan and Lv, Ying and Lyu, Chengqi and Ma, Chenglong and Ma, Jianpeng and Ma, Ren and Ma, Runmin and Ma, Runyuan and Ma, Xinzhu and Ma, Yichuan and Ma, Zihan and Mi, Sixuan and Ning, Junzhi and Ning, Wenchang and Pang, Xinle and Peng, Jiahui and Peng, Runyu and Qiao, Yu},
  journal={CoRR},
  volume={abs/2508.15763},
  year={2025},
  publisher={arXiv},
  preview={text_lan_fusion.png},
  selected={false},
  bibtex_show={true},
  abstract={Intern-S1 presents a scientific multimodal foundation model for various research applications.}
}

@article{hu2025survey,
  abbr={arXiv},
  title={A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers},
  author={Hu, Ming and Ma, Chenglong and Li, Wei and Xu, Wanghan and Wu, Jiamin and Hu, Jucheng and Li, Tianbin and Zhuang, Guohang and Liu, Jiaqi and Lu, Yingzhou and Chen, Ying and Zhang, Chaoyang and Tan, Cheng and Ying, Jie and Wu, Guocheng and Gao, Shujian and Chen, Pengcheng and Lin, Jiashi and Wu, Haitao and Chen, Lulu and Wang, Fengxiang and Zhang, Yuanyuan and Zhao, Xiangyu and Tang, Feilong and Su, Encheng and Ning, Junzhi and Liu, Xinyao and Du, Ye and Ji, Changkai and Tang, Cheng and Xu, Huihui and Chen, Ziyang and Huang, Ziyan and Liu, Jiyao and Jiang, Pengfei and Wang, Yizhou and Tang, Chen and Wu, Jianyu and Ren, Yuchen and Yan, Siyuan and Wang, Zhonghua and Xu, Zhongxing and Su, Shiyan and Sun, Shangquan and Zhao, Runkai and Zhang, Zhisheng and Liu, Yu and Wang, Fudi and Ji, Yuanfeng and Su, Yanzhou and Shan, Hongming and Feng, Chun-Mei and Xu, Jiahao and Yan, Jiangtao and Tang, Wenhao and Song, Diping and Liu, Lihao and Huang, Yanyan and Yu, Lequan and Fu, Bin and Wang, Shujun and Li, Xiaomeng and Hu, Xiaowei and Gu, Yun and Fei, Ben and Deng, Zhongying and Wang, Benyou and Cao, Yuewen and Shen, Minjie and Duan, Haodong and Xu, Jie and Chen, Yirong and Yan, Fang and Hao, Hongxia and Li, Jielan and Du, Jiajun and Wang, Yanbo and Razzak, Imran and Zhang, Chi and Wu, Lijun and He, Conghui and Lu, Zhaohui and Huang, Jinhai and Liu, Yihao and Ling, Fenghua and Li, Yuqiang and Wang, Aoran and Zheng, Qihao and Dong, Nanqing and Fu, Tianfan and Zhou, Dongzhan and Lu, Yan and Zhang, Wenlong and Ye, Jin and Cai, Jianfei and Ouyang, Wanli and Qiao, Yu and Ge, Zongyuan and Tang, Shixiang and He, Junjun},
  journal={CoRR},
  volume={abs/2508.21148},
  year={2025},
  publisher={arXiv},
  preview={scientific_llm_survey.png},
  selected={false},
  bibtex_show={true},
  abstract={A comprehensive survey of scientific large language models examining their development from data foundations to advanced agent applications across various scientific domains.},
  html={https://arxiv.org/abs/2508.21148}
}

@mastersthesis{ning2024armut,
  abbr={Imperial},
  title={ARMUT-LOR: Adaptive Region-aware Masked Unpaired Translation for Lung Opacity Removal in Chest X-rays},
  author={Ning, Junzhi},
  school={Imperial College London},
  year={2024},
  preview={cxr_lung_opacity.png},
  selected={false},
  bibtex_show={true},
  abstract={Master's research project focusing on adaptive region-aware masked unpaired translation for lung opacity removal in chest X-rays.}
}
